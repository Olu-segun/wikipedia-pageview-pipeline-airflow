## Project Description

This project implements an Apache Airflowâ€“orchestrated ETL pipeline to automate the extraction, transformation, and loading of Wikipedia pageview data.
The pipeline tracks hourly pageview activity for selected global technology companies â€” Apple, Amazon, Facebook, Google, Microsoft, Tesla, IBM, and Oracle â€” and stores the processed results in a PostgreSQL database for analysis.

The goal of this project is to demonstrate end-to-end data pipeline development, including data ingestion from external sources, transformation logic, database loading, and analytical querying.

---

## Tech Stack

    â€¢	Orchestration: Apache Airflow

    â€¢	Programming Language: Python 3.11

    â€¢	Database: PostgreSQL

    â€¢	Data Processing: Pandas

    â€¢	Infrastructure: Docker & Docker Compose

    â€¢	Data Source: Wikimedia Pageviews Dumps
---
 ## Project Workflow
 
    1.	Extract

        â€¢   Downloads hourly Wikipedia pageview data (compressed .gz format) for a specified hour in December 2025.

        â€¢   Stores the raw file locally for processing.

    2. Transform

        â€¢   Decompresses and parses the dataset.

        â€¢   Filters records to include only predefined companies of interest.

        â€¢   Aggregates total pageviews per company.

    3. Load

        â€¢   Inserts the transformed dataset into a PostgreSQL table.

        â€¢   Ensures the target table exists before loading.

    4. Analyze

        â€¢   Runs SQL queries to identify engagement trends.

    â€¢   Determines the company with the highest pageview count for the selected time window
---
### ğŸ“ Repository Structure
<pre>
Wikipedia-Pageview-Data-Pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ wikipedia_company_views_etl_pipeline.py   # Main Airflow DAG
â”‚   â”œâ”€â”€ extract_views.py                          # Data extraction logic
â”‚   â”œâ”€â”€ transform_views.py                        # Data transformation logic
â”‚   â”œâ”€â”€ load_views.py                             # Data loading logic
â”‚
â”œâ”€â”€ pyenv/                                        # Python virtual environment
â”œâ”€â”€ logs/                                         # Airflow task logs
â”œâ”€â”€ docker-compose.yaml                           # Airflow & Postgres services
â”œâ”€â”€ requirements.txt                              # Python dependencies
â””â”€â”€ README.md
</pre>
---
## ğŸ“ˆ Logs & Execution Evidence
The following screenshots demonstrate successful pipeline execution and data validation:

    â€¢ Airflow DAG Run:
<p align= "center" > 
    <img src="images/airflow_ui_run.jpeg" width="600"> 
</p>

    â€¢ PostgreSQL Query Results:
        images/query_run.jpeg
---
## ğŸš€ Key Highlights

    â€¢   End-to-end ETL pipeline using Apache Airflow

    â€¢   Real-world external data ingestion (Wikimedia dumps)

    â€¢   Modular, reusable Python code structure

    â€¢   PostgreSQL-based analytical storage

    â€¢   Dockerized environment for easy setup and reproducibility
---
## ğŸ¯ Use Cases

    â€¢   Monitoring public interest trends for major technology companies

    â€¢   Demonstrating data engineering and analytics engineering skills
