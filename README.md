# ğŸ“Š Wikipedia Pageview Data Pipeline (Apache Airflow)
## ğŸ“Œ Project Description

This project implements an Apache Airflowâ€“orchestrated ETL pipeline to automate the extraction, transformation, and loading of Wikipedia pageview data.
The pipeline tracks hourly pageview activity for selected global technology companies â€” Apple, Amazon, Facebook, Google, Microsoft, Tesla, IBM, and Oracle â€” and stores the processed results in a PostgreSQL database for analysis.

The goal of this project is to demonstrate end-to-end data pipeline development, including data ingestion from external sources, transformation logic, database loading, and analytical querying.

---

## ğŸ› ï¸ Tech Stack

-	Orchestration: Apache Airflow

-	Programming Language: Python 3.11

-	Database: PostgreSQL

-	Data Processing: Pandas

-   Infrastructure: Docker & Docker Compose

-	Data Source: Wikimedia Pageviews Dumps
---
 ## Project Workflow
 
1.	Extract

    â€¢   Downloads hourly Wikipedia pageview data (compressed .gz format) for a specified hour in December 2025.

    â€¢    Stores the raw file locally for processing.

2. Transform

    â€¢   Decompresses and parses the dataset.

    â€¢   Filters records to include only predefined companies of interest.

    â€¢  Aggregates total pageviews per company.

3. Load

    â€¢   Inserts the transformed dataset into a PostgreSQL table.

    â€¢   Ensures the target table exists before loading.

4. Analyze

    â€¢  Runs SQL queries to identify engagement trends.

    â€¢  Determines the company with the highest pageview count for the selected time window
---
### ğŸ“ Repository Structure
<pre>
Wikipedia-Pageview-Data-Pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ wikipedia_company_views_etl_pipeline.py   # Main Airflow DAG
â”‚   â”œâ”€â”€ extract_views.py                          # Data extraction logic
â”‚   â”œâ”€â”€ transform_views.py                        # Data transformation logic
â”‚   â”œâ”€â”€ load_views.py                             # Data loading logic
â”‚
â”œâ”€â”€ pyenv/                                        # Python virtual environment
â”œâ”€â”€ logs/                                         # Airflow task logs
â”œâ”€â”€ docker-compose.yaml                           # Airflow & Postgres services
â”œâ”€â”€ requirements.txt                              # Python dependencies
â””â”€â”€ README.md
</pre>
---
## ğŸ“ˆ Logs & Execution Evidence
The following screenshots demonstrate successful pipeline execution and data validation:

### Airflow DAG Run:
<p align= "center" > 
    <img src="images/airflow_ui_run.jpeg" width="800"> 
</p>

### PostgreSQL Query Results:
<p align= "center" > 
    <img src="images/query_run.jpeg" width="800"> 
</p>
        
---
## ğŸš€ Key Highlights

-   End-to-end ETL pipeline using Apache Airflow

-   Real-world external data ingestion (Wikimedia dumps)

-   Modular, reusable Python code structure

-   PostgreSQL-based analytical storage

-   Dockerized environment for easy setup and reproducibility
---
## ğŸ¯ Use Cases

-   Monitoring public interest trends for major technology companies

-   Demonstrating data engineering and analytics engineering skills
